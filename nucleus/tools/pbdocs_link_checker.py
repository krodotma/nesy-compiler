#!/usr/bin/env python3
"""PBDOCS Link Checker - Documentation Link Validation Tool.

DKIN v28 Compliant - Documentation Link Auditing

Crawls all markdown files in docs/ directory and validates:
- Internal links (relative paths to other docs)
- Anchor links (within-page navigation)
- External links (optional, with timeout)
- Image references

Usage:
    python3 nucleus/tools/pbdocs_link_checker.py                    # Quick check (internal only)
    python3 nucleus/tools/pbdocs_link_checker.py --external         # Include external links
    python3 nucleus/tools/pbdocs_link_checker.py --json             # JSON output
    python3 nucleus/tools/pbdocs_link_checker.py --fix              # Suggest fixes
"""
from __future__ import annotations

import argparse
import json
import os
import re
import sys
import urllib.request
from dataclasses import dataclass, field
from pathlib import Path
from typing import Literal
from urllib.error import HTTPError, URLError

sys.dont_write_bytecode = True


@dataclass
class LinkInfo:
    """Information about a single link."""
    source_file: str
    line_number: int
    link_text: str
    link_url: str
    link_type: Literal["internal", "anchor", "external", "image"]
    status: Literal["valid", "broken", "unknown", "skipped"]
    error: str | None = None
    suggestion: str | None = None


@dataclass
class LinkCheckReport:
    """Complete link check report."""
    total_files: int
    total_links: int
    valid_links: int
    broken_links: int
    skipped_links: int
    files_with_issues: list[str] = field(default_factory=list)
    broken: list[LinkInfo] = field(default_factory=list)
    all_links: list[LinkInfo] = field(default_factory=list)

    def to_dict(self) -> dict:
        return {
            "total_files": self.total_files,
            "total_links": self.total_links,
            "valid_links": self.valid_links,
            "broken_links": self.broken_links,
            "skipped_links": self.skipped_links,
            "files_with_issues": self.files_with_issues,
            "broken_details": [
                {
                    "file": b.source_file,
                    "line": b.line_number,
                    "url": b.link_url,
                    "type": b.link_type,
                    "error": b.error,
                    "suggestion": b.suggestion,
                }
                for b in self.broken
            ],
        }

    def to_markdown(self) -> str:
        lines = [
            "# Link Check Report",
            "",
            f"**Total Files:** {self.total_files}",
            f"**Total Links:** {self.total_links}",
            f"**Valid:** {self.valid_links} | **Broken:** {self.broken_links} | **Skipped:** {self.skipped_links}",
            "",
        ]

        if self.broken:
            lines.extend([
                "## Broken Links",
                "",
                "| File | Line | URL | Type | Error |",
                "|------|------|-----|------|-------|",
            ])
            for b in self.broken[:50]:
                url_short = b.link_url[:40] + "..." if len(b.link_url) > 40 else b.link_url
                lines.append(f"| `{b.source_file}` | {b.line_number} | `{url_short}` | {b.link_type} | {b.error or 'N/A'} |")
            if len(self.broken) > 50:
                lines.append(f"| ... | ... | ... | ... | +{len(self.broken) - 50} more |")

            # Suggestions
            suggestions = [b for b in self.broken if b.suggestion]
            if suggestions:
                lines.extend([
                    "",
                    "## Suggested Fixes",
                    "",
                ])
                for s in suggestions[:20]:
                    lines.append(f"- `{s.link_url}` -> `{s.suggestion}` (in {s.source_file}:{s.line_number})")

        lines.extend([
            "",
            "---",
            "*Generated by PBDOCS Link Checker - DKIN v28*",
        ])

        return "\n".join(lines)


class PBDocsLinkChecker:
    """Documentation link validation tool."""

    # Regex patterns for link extraction
    MARKDOWN_LINK = re.compile(r'\[([^\]]*)\]\(([^)]+)\)')
    MARKDOWN_IMAGE = re.compile(r'!\[([^\]]*)\]\(([^)]+)\)')
    HTML_LINK = re.compile(r'<a\s+[^>]*href=["\']([^"\']+)["\'][^>]*>')
    HTML_IMAGE = re.compile(r'<img\s+[^>]*src=["\']([^"\']+)["\'][^>]*>')

    # External link domains to skip (too slow or rate-limited)
    SKIP_DOMAINS = [
        "github.com",  # Rate limits
        "twitter.com",
        "x.com",
        "linkedin.com",
        "example.com",
        "localhost",
        "127.0.0.1",
    ]

    def __init__(self, docs_root: Path | None = None, check_external: bool = False):
        self.docs_root = docs_root or Path("/pluribus/docs")
        self.check_external = check_external
        self.timeout = 5  # seconds for external links

    def extract_links(self, content: str, file_path: Path) -> list[LinkInfo]:
        """Extract all links from markdown content."""
        links: list[LinkInfo] = []
        lines = content.split("\n")

        for line_num, line in enumerate(lines, 1):
            # Markdown images
            for match in self.MARKDOWN_IMAGE.finditer(line):
                alt_text, url = match.groups()
                links.append(LinkInfo(
                    source_file=str(file_path.relative_to(self.docs_root)),
                    line_number=line_num,
                    link_text=alt_text,
                    link_url=url,
                    link_type="image",
                    status="unknown",
                ))

            # Markdown links
            for match in self.MARKDOWN_LINK.finditer(line):
                text, url = match.groups()
                # Skip if already captured as image
                if f"![{text}]({url})" in line:
                    continue
                link_type: Literal["internal", "anchor", "external"] = "internal"
                if url.startswith(("#", "?")):
                    link_type = "anchor"
                elif url.startswith(("http://", "https://", "//")):
                    link_type = "external"
                links.append(LinkInfo(
                    source_file=str(file_path.relative_to(self.docs_root)),
                    line_number=line_num,
                    link_text=text,
                    link_url=url,
                    link_type=link_type,
                    status="unknown",
                ))

            # HTML links (less common in markdown but possible)
            for match in self.HTML_LINK.finditer(line):
                url = match.group(1)
                link_type = "internal"
                if url.startswith(("#", "?")):
                    link_type = "anchor"
                elif url.startswith(("http://", "https://", "//")):
                    link_type = "external"
                links.append(LinkInfo(
                    source_file=str(file_path.relative_to(self.docs_root)),
                    line_number=line_num,
                    link_text="[HTML link]",
                    link_url=url,
                    link_type=link_type,
                    status="unknown",
                ))

        return links

    def resolve_internal_link(self, link: LinkInfo, source_path: Path) -> tuple[bool, str | None, str | None]:
        """Resolve and validate an internal link.

        Returns:
            Tuple of (is_valid, error_message, suggestion).
        """
        url = link.link_url

        # Strip anchor from URL for file resolution
        url_path = url.split("#")[0].split("?")[0]
        if not url_path:
            return True, None, None  # Pure anchor link

        # Handle absolute paths (from docs root)
        if url_path.startswith("/"):
            target = self.docs_root / url_path.lstrip("/")
        else:
            # Relative path
            source_dir = source_path.parent
            target = source_dir / url_path

        # Normalize
        target = target.resolve()

        # Check if target exists
        if target.exists():
            return True, None, None

        # Check with .md extension
        if not target.suffix:
            md_target = target.with_suffix(".md")
            if md_target.exists():
                return True, None, None
            # Check for index.md in directory
            index_target = target / "index.md"
            if index_target.exists():
                return True, None, None

        # Generate suggestion
        suggestion = None
        base_name = target.stem

        # Look for similar files
        search_dir = target.parent if target.parent.exists() else self.docs_root
        if search_dir.exists():
            candidates = list(search_dir.glob(f"*{base_name}*"))
            candidates += list(search_dir.glob("*.md"))
            for c in candidates[:5]:
                if c.is_file() and c.suffix == ".md":
                    rel_path = c.relative_to(source_path.parent) if source_path.parent in c.parents else c.relative_to(self.docs_root)
                    suggestion = str(rel_path)
                    break

        return False, f"File not found: {target}", suggestion

    def check_external_link(self, url: str) -> tuple[bool, str | None]:
        """Check if external link is accessible."""
        # Skip certain domains
        for domain in self.SKIP_DOMAINS:
            if domain in url:
                return True, None  # Skip = assume valid

        try:
            req = urllib.request.Request(
                url,
                headers={"User-Agent": "PBDOCS-LinkChecker/1.0"},
                method="HEAD",
            )
            with urllib.request.urlopen(req, timeout=self.timeout) as response:
                if response.status < 400:
                    return True, None
                return False, f"HTTP {response.status}"
        except HTTPError as e:
            # Try GET if HEAD fails
            try:
                req = urllib.request.Request(url, headers={"User-Agent": "PBDOCS-LinkChecker/1.0"})
                with urllib.request.urlopen(req, timeout=self.timeout) as response:
                    return response.status < 400, None if response.status < 400 else f"HTTP {response.status}"
            except Exception:
                pass
            return False, f"HTTP {e.code}"
        except URLError as e:
            return False, f"URL Error: {e.reason}"
        except Exception as e:
            return False, str(e)

    def check_links(self) -> LinkCheckReport:
        """Check all links in documentation."""
        report = LinkCheckReport(
            total_files=0,
            total_links=0,
            valid_links=0,
            broken_links=0,
            skipped_links=0,
        )

        if not self.docs_root.exists():
            return report

        # Find all markdown files
        md_files = list(self.docs_root.glob("**/*.md"))
        report.total_files = len(md_files)

        files_with_issues: set[str] = set()

        for md_file in md_files:
            try:
                content = md_file.read_text(encoding="utf-8")
            except Exception:
                continue

            links = self.extract_links(content, md_file)
            report.total_links += len(links)

            for link in links:
                if link.link_type == "anchor":
                    # Anchor links - just assume valid for now
                    link.status = "valid"
                    report.valid_links += 1

                elif link.link_type == "internal" or link.link_type == "image":
                    is_valid, error, suggestion = self.resolve_internal_link(link, md_file)
                    if is_valid:
                        link.status = "valid"
                        report.valid_links += 1
                    else:
                        link.status = "broken"
                        link.error = error
                        link.suggestion = suggestion
                        report.broken_links += 1
                        report.broken.append(link)
                        files_with_issues.add(link.source_file)

                elif link.link_type == "external":
                    if not self.check_external:
                        link.status = "skipped"
                        report.skipped_links += 1
                    else:
                        is_valid, error = self.check_external_link(link.link_url)
                        if is_valid:
                            link.status = "valid"
                            report.valid_links += 1
                        else:
                            link.status = "broken"
                            link.error = error
                            report.broken_links += 1
                            report.broken.append(link)
                            files_with_issues.add(link.source_file)

                report.all_links.append(link)

        report.files_with_issues = sorted(files_with_issues)
        return report


def main():
    parser = argparse.ArgumentParser(description="PBDOCS Link Checker")
    parser.add_argument("--docs-root", type=str, default="/pluribus/docs", help="Documentation root directory")
    parser.add_argument("--external", action="store_true", help="Check external links (slower)")
    parser.add_argument("--json", action="store_true", help="Output as JSON")
    parser.add_argument("--fix", action="store_true", help="Show suggested fixes")

    args = parser.parse_args()

    checker = PBDocsLinkChecker(
        docs_root=Path(args.docs_root),
        check_external=args.external,
    )

    report = checker.check_links()

    if args.json:
        print(json.dumps(report.to_dict(), indent=2))
    else:
        print(report.to_markdown())

    # Exit with error code if broken links found
    sys.exit(1 if report.broken_links > 0 else 0)


if __name__ == "__main__":
    main()
